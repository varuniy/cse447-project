{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Mq4iDUwBsxsE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "with open('../data/training.txt', 'r', encoding='utf-8') as f:\n",
        "    train_text = f.read()\n",
        "\n",
        "with open('../data/validation.txt', 'r', encoding='utf-8') as f:\n",
        "    val_text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bcg7uqINkW3I",
        "outputId": "6613ec3a-0ca4-497b-aa5e-d004457310ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 9408\n",
            "<unk>\n",
            " !\"$%&'()*+,-./0123456789:;<>?ABCDEFGHIJKLMNOPQRSTUVWXYZ`abcdefghijklmnopqrstuvwxyz~¡¢£¤¥¦§¨ª«¬®°±²³´µ¶·¸¹º»¼½¾¿ÀÁÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõö÷øùúûüýþÿĀāĂăĄąĆćĈĉĊċČčĎďĐđĒēĔĕĖėęĚěĜĝĞğĠġĢģĤĥĦħĩĪīĬĭĮįİıĴĵĶķĺĻļĽľŀŁłŃńŅņňŉŋŌōŎŏŐőŒœŕŗŘřŚśŜŝŞşŠšŢţŤťũŪūŬŭŮůŰűųŴŵŷŸŹźŻżŽžſƁƊƏƒƗƘƙơƯưƲƳƴƼǂǎǐǒǔǚǜǝǧǫǵǹȘșȚțȝȫɑɒɓɔɕɗəɛɜɡɣɦɨɪɬɲɵɹɽɾʀʂʃʊʋʌʎʏʒʔʕʙʜʰʷʹʻʼʽʾʿˁˆˇˈˊˌː˙˚˝ˤ˧˩ˮ̡̧̥̯̱̲̀́̂̃̄̆̈̊̌̍͏͘͡΄ΆΈΉΊΌΎΏΐΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩάέήίΰαβγδεζηθικλμνξοπρςστυφχψωϊϋόύώϑϕЁЂЃЄЅІЇЈЉЊЋЌЎЏАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяѐёђѓєѕіїјљњћќѝўџѣѫѳҐґҒғҔҕҖҗҘҙҚқҠҡҢңҥҧҪҫҮүҰұҲҳҶҷҺһӀӄӊӐӑӖӗӘәӢӣӧӨөӮӯӱӲӳԱԲԳԴԵԶԷԸԹԺԻԼԽԾԿՀՁՂՃՄՅՆՇՈՉՊՋՌՍՎՏՐՑՒՓՔՕՖ՛՜՝՞աբգդեզէըթժիլխծկհձղճմյնշոչպջռսվտրցւփքօֆև։֊ְֱֲִֵֶַָֹֻּ־ֿׁׂאבגדהוזחטיךכלםמןנסעףפץצקרשתװױײ׳״؁،؍؎ؐؑؒؓؔ؛؟ءآأؤإئابةتثجحخدذرزسشصضطظعغـفقكلمنهوىيًٌٍَُِّْٖٓٔٗٚ٠١٢٣٤٥٦٧٨٩٪٫٬٭ٰٱٴٶٸٹٺٻټٽپٿڀځڃڄڅچڇڈډڊڌڍڏڑړڕږژڙښڠڡڤڦڧکڪګڭگڱڳڵںڻڼھۀہۂۃۆۇۈۊۋیۍێېےۓ۔ەۖ۞ۡ۰۱۲۳۴۵۶۷۸۹۽۾ܐܒܕܘܝܢܪݨހށނރބޅކއވމފދތލގޏސޑޒޓޔޕޖޗޘޙޚޛޜޝޞޟޠޡޢޣޤޥަާިީުޫެޭޮޯްޱँंःअआइईउऊऋऍऎएऐऑऒओऔकखगघङचछजझञटठडढणतथदधनऩपफबभमयरऱलळऴवशषसह़ऽािीुूृॄॅॆेैॉॊोौ्ॐ॑॓ॠ।॥०१२३४५६७८९॰ॲঁংঃঅআইঈউঊঋএঐওঔকখগঘঙচছজঝঞটঠডঢণতথদধনপফবভমযরলশষসহ়ািীুূৃেৈোৌ্ৎ০১২৩৪৫৬৭৮৯ৰৱ৳৷৺ਂਃਅਆਇਈਉਊਏਐਓਔਕਖਗਘਙਚਛਜਝਞਟਠਡਢਣਤਥਦਧਨਪਫਬਭਮਯਰਲਵਸਹ਼ਾਿੀੁੂੇੈੋੌ੍ੜ੦੧੨੩੪੫੬੭੮੯ੰੱੲੳઁંઃઅઆઇઈઉઊઋઍએઐઑઓઔકખગઘઙચછજઝઞટઠડઢણતથદધનપફબભમયરલળવશષસહ઼ાિીુૂૃૅેૈૉોૌ્ૐૢ૦૧૨૩૪૫૬૭૮૯ଁଂଃଅଆଇଈଉଊଋଏଐଓଔକଖଗଘଙଚଛଜଝଞଟଠଡଢଣତଥଦଧନପଫବଭମଯରଲଳଶଷସହ଼ାିୀୁୂୃେୈୋୌ୍ୖୟ୤୦୧୨୩୪୫୬୭୮୯ୱஃஅஆஇஈஉஊஎஏஐஒஓஔகஙசஜஞடணதநனபமயரறலளழவஷஸஹாிீுூெேைொோௌ்ఁంఃఅఆఇఈఉఊఋఎఏఐఒఓఔకఖగఘఙచఛజఝఞటఠడఢణతథదధనపఫబభమయరఱలళవశషసహాిీుూృెేైొోౌ్౦ಂಃಅಆಇಈಉಊಋಎಏಐಒಓಔಕಖಗಘಙಚಛಜಝಞಟಠಡಢಣತಥದಧನಪಫಬಭಮಯರಲಳವಶಷಸಹ಼ಾಿೀುೂೃೆೇೈೊೋೌ್೦೧೨೩೪೫೬೭೮೯ംഃഅആഇഈഉഊഋഎഏഐഒഓഔകഖഗഘങചഛജഝഞടഠഡഢണതഥദധനപഫബഭമയരറലളഴവശഷസഹ഼ാിീുൂൃെേൈൊോൌ്ൗൺൻർൽൾൿංඃඅආඇඈඉඊඋඌඍඑඒඓඔඕඖකඛගඝඞඟචඡජඣඤඥටඨඩඪණඬතථදධනඳපඵබභමඹයරලවශෂසහළෆ්ාැෑිීුූෘෙේෛොෝෞෟෲกขคงจชณดตถทธนบปผพภมยรลวศษสหอะัาำิีุูเแโไ่้์ຂງນວຫະແ་།ཁགངཆདནཔབམཚའརལསིེོུྐྱྲླကငညတဒနပမသာိီုေး္်ြაბგდევზთიკლმნოპჟრსტუფქღყშჩცძწჭხჯჰჲჳჴჷჸሀሁሂሃሄህሆለሉሊላሌልሎሏሐሑሒሓሔሕሖሗመሙሚማሜምሞሟሠሡሢሣሤሥሦረሩሪራሬርሮሯሰሱሲሳሴስሶሷሸሹሺሻሼሽሾሿቀቁቂቃቄቅቆቈቊቋቍቐበቡቢባቤብቦቧቨቩቪቫቬቭቮተቱቲታቴትቶቷቸቹቺቻቼችቾቿኀኃኄኅኆኋነኑኒናኔንኖኗኘኙኚኛኜኝኞኟአኡኢኣኤእኦኧከኩኪካኬክኮኰኲኳኵኸኹኻኼኽኾዃወዉዊዋዌውዎዐዑዒዓዔዕዖዘዙዚዛዜዝዞዟዠዡዢዣዤዥዦየዩዪያዬይዮደዱዲዳዴድዶዷዻዽጀጁጂጃጄጅጆጇገጉጊጋጌግጎጐጒጓጔጕጘጝጠጡጢጣጤጥጦጧጨጩጪጫጬጭጮጯጰጲጳጴጵጶጸጹጺጻጼጽጾጿፀፁፂፃፄፅፆፈፉፊፋፌፍፎፏፐፑፒፓፔፕፖ፡።፣፤፥፦፩፪፫፬፭፮፯፰፱፲፳፴፵፶፷፸፹፺፻កងនមរសហាុ្᠌ᠠᠨᠩᠭᠸᡝᡠᡳᮤ᮪ᴍᴎᴙᴛᴦᴫᵒᵘḇḍḎḏḐḑḗḡḤḥḨḩḪḫḳḶḷḻṁṃṅṇṉṓṙṚṛṟṡṢṣṬṭṮṯṿẋẐẑẓẕẖạẢảẤấầẩẫậắằẳẵặẹẻẽếềểễệỉịọỏỐốồổỗộớờỞởỡợụỦủỨứừửữựỲỳỵỷỹἀἄἈἉἌἐἑἔἕἘἙἡἨἩἰἱἴἵἶἸἹὀὁὄὅὈὐὑὰὲὴὶὸὺᾱᾶῆῑῖῥῦῬῳῶῷ     ​‌‍‎‏‐‑‒–—―‘’‚‛“”„†‡•․…‧‫‬‭ ‰′″‹›※⁄⁣⁰ⁿ₀₁₂₃₄₅₆₇₈₉₤₨₩₪₫€₱₹₺℃ℓ№ℝ™⅓⅔ⅡⅢⅨⅩ←↑→↓↔↵⇒∀∂∆∈−∗∙∞∴∼≈≠≡≤≥≪≫⊕⋅①②③④⑤─╱□▲△◀○◦☃★☆☉♀♂♔♘♠♡♣♦♭♯⟨⟩ⲁⲉⲟⴰⴳⵉⵍⵎⵏⵓⵔⵜⵟ　、。々〇〈〉《》「」『』【】〔〕〜ぁあぃいうえおかがきぎくぐけげこごさざしじすずせぜそぞただちぢっつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽまみむめもゃやゅゆょよらりるれろわをんゞァアィイゥウェエォオカガキギクグケゲコゴサザシジスズセゼソゾタダチッツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポマミムメモャヤュユョヨラリルレロワヲンヴヵヶ・ーㄧㄹㅅㅇㅏㆍ㎝㎡䴓一丁七丄万丈三上下不与丑专且丕世丘丙业丛东丝丞丟両丢两严並丧个中丰串临丸丹为主丼丽举乂乃久么义之乌乍乎乏乐乒乓乔乖乗乘乙九乞也习乡书买乱乳乾亀亂了亇予争事二于亏云互五井亘亚些亜亞亡交亥亦产亨亩享京亭亮亲亳人亿什仁仄仅仆仇今介仍从仏仑仓仔仕他仗付仙代令以仪们仮仰仲件价任份仿企伉伊伍伎伏伐休众优伙会伝伞伟传伤伦伪伯估伲伴伶伸伺似伽佃但佈位低住佐佑体佔何佗佘余佚佛作佞你佢佣佥佩佬佯佳併佼使侃侄來侈例侍侏侗供依侟侠価侣侦侧侨侪侬侮侯侵侶便係促俄俊俗俘俚保俞俟俠信俣俩俭修俯俱俳俵俶俸俺俾倆倉個倍們倒倖倘候倚借倡倣値倦倩倪倫倭倶倷债值倾偃假偉偏偕做停健側偵偶偷偽偿傀傅傍傑傘備傣储催傭傲傳債傷傾僅僉働像僑僕僖僚僧僭僱僵價僻儀儁儂億儉儒儕儘償儡優儲儿兀允元兄充兆兇先光克兌免兎児兑兒兔党兜入內全兩八公六兮兰共关兴兵其具典兹养兼兽冀内円冈冉冊册再冑冒冕冗写军农冠冢冤冥冨冬冯冰冲决冴况冶冷冻净凄准凉凌凍减凑凛凝几凡凤処凪凭凯凰凱凳凶凸凹出击函凿刀刁刃分切刈刊刍刑划列刘则刚创初删判別刨利刪别刮到制刷券刹刺刻剂剃則削剌前剎剑剔剖剛剝剡剣剤剥剧剩剪副剰割創剽剿劃劇劈劉劍劑力劝办功加务劣动助努劫劭励劲劳労効劾势勁勃勅勇勉勋勒動勗勘務勝勞募勢勤勧勲勳勵勸勺勾勿匀包匆匈匕化北匙匝匠匡匣匪匮匯匹区医匾匿區十千卅升午卉半华协卑卒卓協单卖南単博卜卞占卡卢卦卧卫卯印危即却卵卷卸卻卿厂厄厅历厉压厌厕厘厚厝原厢厥厦厨厩厭厲厳去县参參又叉及友双反収发叔取受变叙叛叟叠叡叢口古句另叩只叫召叭叮可台叱史右叶号司叹叻吁吃各吇合吉吊吋同名后吏吐向吒吓吕吗君吞吟吠否吧吨吩含听启吳吴吵吸吹吻吾呀呂呆呈呉告呋呎呒呕员呢呣呤周呪味呵呼命咀咄和咎咏咐咒咕咖咙咤咦咨咪咬咯咲咳咸咽咾哀品哄哆哇哈哉响哑哔哚員哥哨哩哪哭哮哲哺唄唆唇唐唑唔唤售唯唱唸唾商啊問啓啖啞啟啡啤啥啦啶啸啼喀喃善喇喉喊喋喘喙喚喜喝喧喪喫喬單喰営喷喻嗅嗎嗒嗜嗝嗣嗨嗯嗰嗲嘆嘉嘌嘎嘗嘘嘛嘞嘧嘩嘯嘱嘲嘴嘸噂噌噛器噪噬噴噶噸嚇嚕嚜嚢嚴嚼囃囉囊囗囚四回因囡团団囤囥园困囲図围固国图囿圃圆圈國圍圏園圓圖團土圣圧在圩圭地圳场圾址坂均坊坍坎坏坐坑块坚坛坝坞坟坠坡坤坦坪坵垂垃垄型垒垓垢垣垦垩垫垮埃埋城埔埕域埠埭埴執培基埼堀堂堅堆堕堡堤堪堰報場堵堺塁塊塌塑塔塗塘塚塞塢塩填塵塾境墅墊墓増墙墜增墟墨墩墮墳墾壁壇壊壌壓壕壘壞壟壤壩士壬壮壯声売壳壶壹壺壽处备変复夏夔夕外多夜够夠夢夥大天太夫夭央失头夷夸夹夺夾奄奇奈奉奋奎奏契奔奕奖套奘奚奠奢奥奧奨奪奮女奴奶奸她好如妃妄妆妇妈妊妒妓妖妙妝妥妨妬妮妹妻妾姆姉姊始姐姑姓委姚姜姥姦姨姪姫姬姻姿威娃娄娅娇娑娘娛娜娟娠娥娱娴娶娼婁婆婉婚婢婦婭婴婷婺婿媒媚媛媳媽嫁嫂嫉嫌嫔嫖嫡嫦嫩嫲嬉嬌嬢嬪嬰嬴嬷子孔孕字存孙孚孛孜孝孟孢季孤学孩孫孵學孺宁它宅宇守安宋完宏宕宗官宙定宛宜宝实実宠审客宣室宥宦宪宫宮宰害宴宵家宸容宽宾宿寂寄寅密寇富寒寓寔寛寝察寡寢寥實寧寨審寫寬寮寰寵寶寸对寺寻导対寿封専射将將專尉尊尋對導小少尔尖尘尙尚尝尤尧尬就尷尸尹尺尻尼尽尾尿局屁层居屆屈屉届屋屍屎屏屑展屙属屠屡屢層履屬屯山屿岀岁岂岐岑岖岗岙岚岛岡岩岫岬岭岱岳岷岸峃峒峙峠峡峦峨峪峭峯峰島峻峽崁崂崇崎崑崔崖崗崙崛崩嵊嵌嵐嵩嶋嶷嶺嶼嶽巅巍巖川州巡巢巣工左巧巨巩巫差巯己已巳巴巷巻巽巾币市布帅帆师希帐帕帖帘帚帛帜帝帥带帧師席帮帯帰帳帶常帽幀幂幅幌幔幕幟幡幢幣幫干平年并幸幹幻幼幽幾广庁広庄庆庇床序庐库应底店庙庚府庞废度座庫庭庵庶康庸庾廁廂廃廈廉廊廓廖廚廟廠廢廣廬廳延廷建廻廿开弁异弃弄弈弊弋式弑弒弓弔引弗弘弟张弥弦弧弩弯弱張強弹强弼弾彈彌彎归当录彗彙彝形彥彦彩彪彫彬彭彰影彷役彻彼彿往征径待很律後徐徑徒従徕得徙從徠御復循微徳徴徵德徹徽心必忆忌忍忏忒忖志忘忙応忠忧快念忽怀态怎怒怕怖怜思怠怡急性怨怪怯总恁恆恋恍恐恒恕恙恚恢恤恥恨恩恪恬恭息恰恵恶恺恼悅悉悍悔悖悚悟悠患悦您悩悪悬悲悼情惊惑惘惚惜惟惠惡惣惧惨惩惯惰惱想惹愁愈愉意愕愚愛感愤愧愷愿慈態慌慎慕慘慢慣慧慨慮慰慶慾憂憎憐憑憚憤憧憩憬憲憶憾懂懇應懋懐懒懲懷懸懺懼懿戀戈戊戌戍戎戏成我戒或战戚戛戟戦截戮戯戰戲戳戴戶户戸戻房所扁扇扈扉手才扎扑打扔払托扣执扩扫扬扭扮扰扱扳扶批扼找承技抄把抑抒抓投抖抗折抚抛抜択抢护报抨披抬抱抵抹押抽拂担拆拇拉拋拌拍拐拒拓拔拖拗拘拙招拜拝拟拠拡拢拥拦拨择括拭拯拱拳拶拷拼拾拿持挂指按挑挖挙挚挝挟挡挤挥挨挪挫振挺挽挿捆捉捍捏捐捕捜捞损换捣捧捨据捲捷捻掀掃授掉掌掏排掖掘掙掛掟掠採探接控推掩措掲掴掷掺掾揃揄揆揉描提插揚換握揭揮援揶揺揽損搏搖搜搞搬搭搶携搾搿摂摄摆摇摊摔摘摧摩摯摸摹摺撃撇撐撑撒撓撕撞撤撥撫播撮撰撲撼撿擁擂擄擅擇擊擋操擎擒擔據擠擢擦擬擱擲擴擺擾攀攔攘攜攝攤攬支收攸改攻放政故效敌敎敏救敕敖敗敘教敛敞敢散敦敬数敲整敵敷數斃文斉斋斌斎斐斑斗料斜斡斤斥斧斩斬断斯新斷方於施旁旅旋旌族旗无既日旦旧旨早旬旭旱时旷旺昂昆昇昊昌明昏易昔昕昙星映春昧昨昭是昰昴昵昶昼显時晃晉晋晌晏晒晓晔晕晖晚晝晟晤晦晨晩普景晰晴晶智暁暂暇暈暐暑暖暗暢暦暨暫暮暱暴暹曆曇曉曖曙曜曝曦曰曲曳更書曹曼曽曾替最會朆月有朋服朔朕朗望朝朞期木未末本札术朱朴朵机朽杀杂权杆杉李杏材村杓杖杜杞束杠条来杨杭杯杰東杲杵杼松板极构枉析枕林枚果枝枠枢枣枪枫枯架柄柊柏某柑染柔柚柜柠查柬柯柱柳柴柵査柿栃栄栅标栈栋栎栏树栓栖栗校株栱样核根格栽栾桁桂桃框案桌桐桑桓桜桢档桥桦桨桩桶桿梁梅梓梗條梦梧梨梭梯械梳梵梶检棄棉棋棍棒棕棘棚棟棠棣森棱棲棵棺椅椋植椎椒検椭椰椿楊楓楔楚楞楠楨業楯極楷楼楽概榄榆榈榊榕榜榭榮榴槃槊構槌槍槐槓様槙槟槳槻槽樁樂樊樋樑樓標樞樟模樣権横樫樱樵樸樹樺樽橄橇橋橘橙機橡橢橫檀檄檐檔檢檬檻櫃櫛櫻欄權欖欠次欢欣欧欲欺欽款歆歇歉歌歎歐歓歙歡止正此步武歧歩歪歯歲歳歴歷歸歹死歼殆殉殊残殖殘殡殲殴段殷殺殻殼殿毀毁毅毆毋母毎每毒毓比毕毗毘毙毛毫毯氏民气気氛氟氢氣氦氧氨氫氮氯氰水氷永氹氾汀汁求汇汉汎汏汐汕汗汚汝汞江池污汤汪汰汲汴汶汹決汽汾沁沂沃沅沈沉沌沐沒沖沙沛沟没沢沥沦沧沪沫沮河沸油治沼沾沿況泄泉泊泌泓法泗泛泡波泣泥注泪泰泳泵泻泼泽泾洁洋洒洗洛洞津洩洪洮洲洶活洼洽派流浄浅浆浊测济浏浑浓浔浙浚浜浣浦浩浪浮浴海浸涂涅涇消涉涌涓涕涙涛涟涡润涧涨涩涯液涵涸涼淀淄淆淋淑淘淚淞淡淤淨淪淫淮深淳淵混淸淹淺添清済渉渊渋渎渐渓渔渕渗渚減渝渠渡渣渤渥渦温測渭港渲渴游渾湄湊湍湖湘湛湧湯湾湿満溃溉源準溜溝溢溥溧溪溫溯溴溶溺滁滂滄滅滇滋滑滕滚滝滞满滤滥滨滩滬滯滲滴滸滾滿漁漂漆漏演漕漠漢漣漪漫漬漱漲漳漸漾漿潍潔潘潛潜潟潢潤潭潮潰潼澄澈澎澜澤澪澱澳激濁濃濒濕濞濟濠濡濤濫濬濮濱濾瀉瀏瀑瀕瀛瀧瀨瀬瀾灌灑灘灣火灭灯灰灵灶灼災灾灿炀炅炉炊炎炒炙炬炭炮炯炳炸点為炼烂烃烈烏烛烟烤烦烧热烯烷烹焉焊焕焙焚無焦焰然焼煉煌煎煙煜煞煤煥煦照煩煮煽熊熏熔熙熟熬熱熵熹燁燃燈燒燕營燥燭燮爆爐爛爪爬爭爱爲爵父爷爸爹爺爻爽爾爿牆片版牌牍牒牙牛牟牠牡牢牦牧物牲牵特牺牽犀犁犠犧犬犯状犹狀狂狄狐狗狙狛狠狡狩独狭狮狱狸狹狼猎猛猜猟猥猩猪猫猬献猴猶猾猿獄獅獎獠獣獨獲獵獸獻獾玄率玉王玘玛玩玫玮环现玲玳玷玺玻珀珂珅珈珊珍珞珠珪班珲現球琅理琉琊琛琦琪琳琴琵琶琼瑁瑕瑙瑚瑛瑜瑞瑟瑠瑤瑪瑰瑲瑶瑾璃璋璐璘璜璧璨環璽瓊瓘瓜瓣瓦瓯瓶瓷甄甌甘甚甜生產産甥甦用甫甬田由甲申电男甸町画畀畅界畏畑畔留畜畝畠畢略番畫異畲畳畴畵當畸畿疆疇疊疎疏疑疗疟疡疤疫疮疯疲疹疼疾病症痒痕痛痢痩痫痰痴痺瘋瘍瘓瘟瘡瘤瘦瘾療癇癌癒癖癫癱癸発登發白百皂的皆皇皈皋皓皖皮皺皿盃盆盈益盎盐监盒盔盖盗盘盛盜盟盡監盤盧盪目盱盲直相盼盾省眉看県眙眞真眠眶眷眺眼眾着睑睛睡督睦睨睹睿瞄瞋瞎瞒瞞瞬瞭瞰瞳瞻矛矢矣知矩矫短矮石矶矿码砂砌砍研砕砖砥砦砲破砷砸砾础硅硕硝硫硬确硼碁碉碌碍碎碑碓碗碘碟碧碩碰碱碳確碼磁磅磊磐磚磡磨磯磷磺礁礎礙礦礫示礼礽社祀祁祂祇祈祉祐祖祗祚祝神祟祠祥票祭祯祷祸祺祿禀禁禄禅禍禎福禦禧禪禮禱禹禺离禽禾禿秀私秃秉秋种科秒秘租秤秦秩积称移稀稅程稍税稔稚稜稟稠稣種稱稲稳稷稻稼稽稿穀穂穆穌積穎穏穗穢穩穫穴究穷穹空穿突窃窄窑窒窓窗窘窜窝窟窥窦窩窪窮窯窺窿竄竅竊立竖站竜竞竟章竣童竪竭端競竹竺竿笃笈笋笑笔笙笛笠符笨第笹笼筆等筋筏筐筑筒答策筠筱筵筹签简箇箔箕算管箭箱箴節範篆篇築篠篡篤篮簒簡簧簪簽簿籃籌籍籠籤籬籲米类籽粉粋粒粗粘粛粟粤粥粧粮粲粵粹粽精糊糕糖糙糜糟糠糧糸系糾紀約紅紇紋納紐純紗紘紙級紛素紡索紧紫紮累細紳紹紺終絃組絆経結絕絞絡給絨統絲絵絶絹綁綏經継続綜綝綠綢綫綬維綱網綴綺綻綽綾綿緊緋総緑緒線緝締緣編緩緬緯練緻縁縄縛縣縦縫縮縱總績繁繊繋織繕繞繡繩繪繫繭繰繳繹繼纂續纏纔纖纜纠红纤约级纪纫纬纯纱纲纳纵纷纸纹纺纽线练组绅细织终绍绎经绑绒结绕绘给绛络绝绞统绢绣绥继绩绪绫续绰绳维绵绸综绿缀缅缆缉缎缓缔编缘缚缝缠缩缪缮缴缶缸缺罂罐网罔罕罗罚罠罢罩罪置罰署罵罷罹罽羁羅羊羌美羚羞羟羣群羧羨義羯羲羹羽翁翅翊翌習翔翕翟翠翡翰翳翻翼耀老考者耆而耍耐耕耗耦耳耶耸耻耽耿聂聆聊聋职联聖聘聚聞聡聪聯聰聲聴聶職聽肃肅肆肇肉肋肌肖肘肚肛肝肠股肢肤肥肩肪肯育肴肺肽肾肿胀胁胃胄胆背胎胖胚胜胞胡胤胥胭胰胱胴胶胸胺能脂脅脆脇脈脉脊脏脐脑脓脖脚脩脫脯脱脳脸脹脾腊腋腎腐腓腔腕腥腦腫腭腰腱腳腸腹腺腻腾腿膀膏膑膚膛膜膝膠膨膳膺膽膿臀臂臉臓臘臟臣臥臧臨自臭至致臺臻臼舅舆與興舉舊舌舍舎舐舒舖舗舜舞舟航般舰舱舵舶舷船艇艏艘艙艦艮良艰艱色艳艶艷艺艾节芋芒芙芜芝芥芦芬芭芮芯花芳芷芸芹芽苇苍苏苑苔苗苛苞苟若苦苯英苷苹苻茂范茄茅茉茎茗茜茨茫茱茲茴茵茶茹荀荃荆草荊荐荒荔荘荟荡荣荨荫药荷荻荼莆莉莊莎莒莓莖莘莞莪莫莱莲获莽菁菅菊菌菓菜菩華菱菲菸萃萄萊萌萍萎萝萤营萧萨萩萬萱萼落葉著葛葡董葦葬葱葵蒂蒋蒐蒔蒙蒜蒲蒸蒼蓄蓉蓋蓝蓟蓬蓮蔑蔓蔗蔚蔡蔣蔥蔬蔭蔵蔷蔻蔽蕃蕉蕊蕎蕨蕩蕭蕴蕾薄薇薔薙薛薦薨薩薪薫薬薮薯薰藉藍藏藓藝藤藥藩藻蘆蘇蘊蘋蘭蘿虎虏虐虑虔處虚虛虜虞號虢虧虫虹虻虽虾蚀蚁蚊蚌蚕蚤蚩蛇蛉蛋蛍蛎蛙蛛蛤蛮蛾蜀蜂蜃蜒蜓蜗蜘蜜蜡蜥蜴蜻蜿蝇蝉蝎蝕蝗蝙蝠蝦蝴蝶螂融螢螺蟆蟒蟬蟲蟹蟻蠕蠟蠡蠢蠶蠻血衆行衍術衔街衙衛衝衞衡衢衣补表衫衬衰衷衹袁袂袈袋袍袖袜被袭裁裂装裏裔裕裘裙補裝裟裡裤裨裳裴裸裹製裾複褐褒褔褲褶襄襟襪襯襲西要覆覇見規視覗覚覧親観覺覽觀见观规视览觉觐角解触觸言訂計訊討訓託記訛訝訟訣訪設許訳訴訶診註証詐詔評詛詞詠詢詣試詩詫詮詰話該詳詹誅誇誉誌認誓誕誘語誠誣誤誥誦說説読誰課誼調談請諏論諜諡諦諧諫諭諮諴諷諸諺諾謀謂謎謗謙講謝謠謡謨謬謳謹證譏識譚譜警譬譯議譲譴護譽讀讃變讐讓讚计订认讨让训议讯记讲讳讴讷许讹论讼讽设访诀证诃评识诈诉诊词诏译试诗诘诚诛话诞诠诡询诣该详诫诬语误诰诱说诵请诸诺读诽课谁调谅谈谊谋谍谎谏谐谓谔谕谚谜谟谢谣谤谥谦谨谬谭谱谴谶谷谿豁豆豈豊豎豐豚象豪豫豬豹豺貂貌貓貘貝貞負財貢貤貧貨販貪貫責貯貰貴貶買貸費貼貿賀賂賃賄資賈賊賑賓賛賜賞賠賢賣賤賦質賭賴賺購賽贄贅贈贊贏贖贛贝贞负贡财责贤败账货质贩贪贫贬购贮贯贰贱贴贵贷贸费贺贼贾贿资赋赌赎赏赐赔赖赚赛赞赠赢赣赤赦赫走赴赵赶起趁超越趋趕趙趟趣趨足趾跃跋跌跍跑跛距跟跡跤跨跪路跳践跻踊踏踐踞踢踩踪蹄蹈蹟蹤蹲蹴躁躍身躬躯躲躺軀車軋軌軍軒軟転軸軽較載輔輕輛輝輩輪輯輸輻輿轄轉轎轟轢车轨轩转轮软轰轴轻载轿较辅辆辈辉辍辐辑输辖辗辛辜辞辟辣辦辨辩辭辯辰辱農边辺辻込辽达辿迁迂迄迅过迈迎运近返还这进远违连迟迥迦迪迫迭述迴迷迹追退送适逃逅逆选逊逍透逐递途逗這通逝逞速造逢連逮週進逸逻逼逾遁遂遅遇遊運遍過遏道達違遗遙遜遞遠遡遣遥適遭遮遲遵遷選遺遼遽避邀邁邂還邊邏邑邓邕邢那邦邨邪邮邯邱邳邵邸邹邺邻邽郁郊郎郑郝郞郡郢部郭郵郷郸都鄂鄉鄒鄙鄞鄢鄧鄭鄯鄰鄱鄴酉酋酌配酒酔酚酢酥酪酬酮酯酰酱酵酶酷酸酿醇醉醋醍醐醒醚醛醜醤醫醬醯醴醸釀釁采釈釉释釋里重野量釐金釘釜針釣釧鈍鈔鈕鈞鈴鈾鈿鉄鉅鉛鉢鉤鉱鉴銀銃銅銑銘銜銭銳銷鋁鋒鋪鋭鋳鋸鋼錄錐錘錢錦錫錬錯録鍊鍋鍛鍵鍾鎂鎊鎌鎖鎧鎭鎮鎳鏈鏡鐘鐵鐸鑄鑑鑒鑫鑰鑲鑽鑿钆针钉钊钐钓钗钙钝钞钟钠钢钥钦钧钨钩钮钱钳钴钵钻钽钾钿铀铁铃铅铉铌铎铜铝铟铭银铸铺链销锁锅锋锌锐错锚锡锢锣锤锥锦锭键锯锰锺锻镁镇镍镑镕镖镜镠镧镰镶長长門閃閉開閏閑閒間閔閘関閣閤閥閦閩閭閱閲閹閻闆闇闊闕闖闘關闡闢门闪闭问闯闰闲间闵闷闸闹闻闽闾阀阁阅阊阎阐阔阖阙阜队阡阪阮阱防阳阴阵阶阻阿陀陂附际陆陇陈陋陌降限陕陛陝陞陡院陣除陥陨险陪陰陳陵陶陷陸険陽隅隆隈隊隋隍階随隐隔隕隘隙際障隠隣隧隨險隱隶隷隸隻隼难雀雁雄雅集雇雉雌雍雏雑雒雕雖雙雛雜雞離難雨雪雯雰雲雳零雷電雾需霄震霉霊霍霖霜霞霧露霸霹靂靈靑青靖静靜非靠靡面革靭靴靶靼鞅鞋鞍鞏鞑鞘鞠鞭鞮韋韌韓韦韧韩韬音韵韶韻響頁頂頃項順須頌預頑頒頓頗領頜頭頸頻頼顆題額顎顏顔顕願顛類顧顯顱页顶顷项顺须顽顾顿颁颂预颅领颇颈颌颍频颖颗题颚颜额颠颤風颱飆风飓飘飙飛飞食飢飯飲飼飽飾餃餅養餌餐餓餘餡館餵饅饋饑饒饥饪饭饮饰饱饲饶饷饼饿馀馅馆馈馏首香馥馨馬馮馱馳馴駁駄駅駆駐駒駕駛駝駱駿騎騒験騙騨騰騷驅驊驕驗驚驛驟马驯驰驱驳驶驸驻驼驾驿骁骂骄骆验骏骑骗骚骠骤骨骸骼髋髓體高髦髪髭髮鬆鬚鬣鬥鬧鬱鬼魁魂魄魅魏魔魚魯鮑鮫鮮鯉鯖鯨鰓鰭鱗鱷鱼鲁鲍鲜鲤鲨鲸鳃鳄鳌鳍鳖鳗鳞鳥鳩鳳鳴鴉鴦鴨鴻鵜鵝鵡鵬鵲鶏鶯鶴鷗鷲鷹鷺鸚鸟鸠鸡鸢鸣鸥鸦鸭鸯鸳鸽鸿鹅鹉鹏鹘鹤鹦鹫鹬鹭鹰鹵鹹鹼鹽鹿麒麓麗麟麥麦麴麵麺麻麼麾黃黄黎黏黑黒黔默黙黛黜點黟黥黨黯黴鼎鼓鼠鼩鼬鼻齊齋齎齐齒齡齢齣齦齿龄龍龐龔龙龚龜龟꧀가각간갇갈감갑값갓갔강갖같갚개객갤갯갱갸걀거걱건걷걸검겁것겉게겐겔겠겨격겪견결겸겹겼경곁계고곡곤곧골곰곱곳공곶과곽관괄광괘괜괴굉교구국군굳굴굵굶굽굿궁권궐궤귀귄규균그극근글긁금급긋긍기긴길김깃깅깊까깎깐깔깜깝깡깥깨깬깼꺼꺾껍껏껑께껴꼈꼬꼭꼴꼽꽁꽂꽃꽝꽤꾀꾸꾼꿀꿈꿔꿨꿰뀌뀐뀔끄끈끊끌끓끔끗끝끼낀낄낌나낙낚난날낡남납낫났낭낮낯낱낳내낸낼냄냅냈냉냐냥너넉넌널넓넘넛넣네넥넨넬넷녀녁년념녔녕노녹논놀놈농높놓뇌뇨뇽누눅눈눌눠뉘뉜뉴늄느늑는늘늙늠능늦늬니닉닌닐님닙닛닝다닥닦단닫달닭닮닳담답닷당닻닿대댁댄댐댓더덕던덜덟덤덥덧덩덮데덱덴델뎀뎌도독돈돋돌돔돕돗동돛돼됐되된될됨됩두둑둔둘둠둥둬뒀뒤뒷듀듈드득든듣들듬듭듯등디딕딘딛딜딧딩딪따딱딴딸땀땄땅때떠떡떤떨떻떼또똑뚜뚝뚫뚱뛰뜨뜬뜯뜰뜸뜻띄띠띤라락란랄람랍랐랑래랙랜램랩랫랬랭랴략량러럭런럴럼럽럿렀렁렇레렉렌렐렘렛려력련렬렴렵렷렸령례로록론롤롬롭롯롱뢰료룡루룩룬룰룸룹룻룽뤄뤘뤼류륙륜률륨륭르른를름릅릇릉릎리릭린릴림립릿링마막만많맏말맑맘맛망맞맡매맥맨맵맷맹맺머먹먼멀멈멋멍메멕멘멜멤멧며멱면멸명몇모목몫몬몰몸몹못몽뫼묘무묵묶문묻물뭄뭇뭉뭐뭔뮈뮌뮐뮤뮬뮴므미믹민믿밀밈밋밍및밑바박밖반받발밝밟밤밥밧방밭배백밴밸뱀뱃뱅뱉버벅번벌범법벗벙베벡벤벨벳벵벼벽변별볍병보복본볼봄봇봉봐봤뵈부북분불붉붐붓붕붙뷔뷰브븐블비빅빈빌빔빗빙빚빛빠빨빵빼빽뺀뺏뻐뻔뻗뼈뽑뾰뿌뿐뿔뿜쁘쁜쁠쁨삐사삭산살삶삼삽삿샀상새색샌샘생샤샨샬샵샷샹섀서석섞선섣설섬섭섯섰성세섹센셀셈셉셋셔션셜셨셰셸소속손솔솜솟송솥쇄쇠쇼숀숍수숙순술숨숫숭숲쉐쉬쉽슈슐슘슛스슨슬슭슴습슷승시식신싣실싫심십싯싱싶싸싹싼쌀쌍쌓써썩썬썼쏘쏟쏠쑤쑥쓰쓴쓸씀씌씨씩씬씹씻아악안앉않알앓암압앗았앙앞애액앤앨앰앱앵야약얀얄얇양얕얘어억언얹얻얼얽엄업없엇었엉엌엎에엑엔엘엠엡엣여역엮연열염엽엿였영옆예옌옐옛오옥온올옮옳옴옵옷옹와완왈왓왔왕왜외왼요욕용우욱운울움웃웅워웍원월웠웨웬웰웹위윅윈윌윗윙유육윤율융으윽은을읊음읍응의이익인일읽잃임입잇있잉잊잎자작잔잖잘잠잡잣장잦재잭잼쟁저적전절젊점접정젖제젝젠젤젯져졌조족존졸좀좁종좋좌죄죠주죽준줄줌줍중줘줬쥐쥔쥬즈즉즌즐즘증지직진질짐집짓징짖짙짚짜짝짧짱째쨌쩌쩍쩔쪼쪽쫓쭉쯤찌찍찐찔찢차착찬찮찰참창찾채책챈챌챔챙챠처척천철첨첩첫청체첸첼쳐쳤초촉촌촐촘촛총촬최쵸추축춘출춤충춰취츠측츰층치칙친칠침칩칭카칸칼캄캉캐캔캘캠캡커컨컫컬컴컵컷컸케켄켈켓켜켰코콕콘콜콤콥콧콩콰쾌쿄쿠쿤쿨쿼퀀퀄퀘퀴퀸큐크큰클큼키킥킨킬킴킷킹타탁탄탈탐탑탓탔탕태택탠탤탭탱터턱턴털텀텃텅테텍텐텔템텝토톡톤톨톰톱통퇴투툰툴툼퉁튀튜트특튼틀틈티틱틴틸팀팅파팍판팔팜팝팟팡패팩팬팽퍼퍽펀펄펌펑페펙펜펠펩펫펭펴편펼폄평폐포폭폰폴폼퐁표푸푹푼풀품풋풍퓨프픈플픔피픽핀필핌핍핏핑하학한할함합핫항해핵핸햄햇했행향허헌헐험헛헝헤헥헨헬혀혁현혈혐협혔형혜호혹혼홀홈홉홋홍화확환활황회획횟횡효후훈훌훔훗훨훼휘휠휩휴흉흐흑흔흘흙흠흡흥흩희흰히힌힐힘힙ﬁﭘﭤﭼﮐﮬﮯﯘﯚﯨﯩﯰﯽﯾ﴾﴿ﷲﷺﷻ︰﹔ﺁﺅﺉﺋﺍﺎﺑﺖﺗﺘﺟﺣﺧﺩﺪﺭﺮﺯﺳﺷﺸﻏﻐﻗﻟﻠﻣﻤﻥﻦﻧﻨﻩﻪﻫﻭﻮﻰﻱﻲﻳﻴﻻ﻿！＂％＆（）＋，－．／０１２３４５６７８：；＜＝＞？ＡＭＳＴ［］ｉｍｔ｜～｡｢｣､･￼\n",
            "Functions saved successfully.\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import pickle\n",
        "\n",
        "# Define the minimum frequency threshold for a character to be considered frequent\n",
        "min_freq = 5  # Characters appearing less than this many times will be replaced by <unk>\n",
        "\n",
        "# Count character frequencies\n",
        "char_counts = Counter(train_text)\n",
        "\n",
        "# Filter out infrequent characters and create a vocabulary\n",
        "freq_chars = [ch for ch, count in char_counts.items() if count >= min_freq]\n",
        "chars = ['<unk>'] + sorted(freq_chars)\n",
        "vocab_size = len(chars)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(''.join(chars))\n",
        "\n",
        "# Create mappings from characters to integers\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}  # String to index\n",
        "itos = {i: ch for i, ch in enumerate(chars)}  # Index to string\n",
        "\n",
        "encode = lambda s: [stoi.get(c, stoi['<unk>']) for c in s]  # Replace unknown chars with <unk>\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "with open('../work/vocab_mappings.pkl', 'wb') as f:\n",
        "    pickle.dump({'stoi': stoi, 'itos': itos, 'vocab_size': vocab_size}, f)\n",
        "\n",
        "print(\"Functions saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7leiMVqsxsJ",
        "outputId": "b7d044e7-f78f-4e70-a086-0c14b26da901"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([333006527]) torch.int64\n",
            "torch.Size([36962966]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "train_data = torch.tensor(encode(train_text), dtype=torch.long)\n",
        "val_data = torch.tensor(encode(val_text), dtype=torch.long)\n",
        "print(train_data.shape, train_data.dtype)\n",
        "print(val_data.shape, val_data.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXfQfn85sxsK",
        "outputId": "e897cce7-31f8-4cab-d32c-d5e9c0191c74"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39mblock_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mestimate_loss\u001b[39m():\n\u001b[0;32m     11\u001b[0m     out \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     12\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "    def generate_top_3(self, idx):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        # Crop to the last block_size tokens\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        logits, loss = self(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        logits = logits[:, -1, :]  # (B, C)\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "\n",
        "        # Get the top 3 most probable tokens\n",
        "        top_probs, top_indices = torch.topk(probs, 3, dim=-1)  # (B, 3)\n",
        "\n",
        "        return top_indices  # Return the top 3 tokens (B, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "checkpoint = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict()\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, 'model_checkpoint.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn3fBiHT9rV-",
        "outputId": "31b2cb71-3eb5-4195-e2b4-9d367ebe0ffa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ai\n"
          ]
        }
      ],
      "source": [
        "input_text = \"Many neurolog\"\n",
        "input_indices = torch.tensor(encode(input_text), dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "generated_indices = m.generate_top_3(input_indices)\n",
        "\n",
        "# Decode the output indices to text (use your decoding logic)\n",
        "generated_text = decode(generated_indices[0].tolist())\n",
        "print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
